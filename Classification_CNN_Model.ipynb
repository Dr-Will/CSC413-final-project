{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC413 Final Project: Classification\n",
    "\n",
    "\n",
    "\n",
    "In this assignment, we will\n",
    "classify \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models\n",
    "import torchvision.datasets\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "creatures_data_path = './resized_images/Flicker8k_Dataset'\n",
    "items_data_path = './resized_images/images'\n",
    "images_data_path = './resized_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "tensor([[[0.3843, 0.4314, 0.4784,  ..., 0.0235, 0.0353, 0.0314],\n",
      "         [0.3608, 0.4667, 0.4902,  ..., 0.0235, 0.0471, 0.0392],\n",
      "         [0.3333, 0.5098, 0.4941,  ..., 0.0392, 0.0588, 0.0588],\n",
      "         ...,\n",
      "         [0.6627, 0.6588, 0.7490,  ..., 0.6627, 0.6588, 0.6588],\n",
      "         [0.6902, 0.8863, 0.9176,  ..., 0.6627, 0.6667, 0.6706],\n",
      "         [0.7569, 0.7098, 0.7333,  ..., 0.6588, 0.6627, 0.6824]],\n",
      "\n",
      "        [[0.3882, 0.4314, 0.4745,  ..., 0.0353, 0.0471, 0.0431],\n",
      "         [0.3647, 0.4706, 0.4863,  ..., 0.0431, 0.0588, 0.0588],\n",
      "         [0.3412, 0.5137, 0.4863,  ..., 0.0667, 0.0784, 0.0863],\n",
      "         ...,\n",
      "         [0.4000, 0.4078, 0.4941,  ..., 0.7373, 0.7216, 0.7216],\n",
      "         [0.4392, 0.6431, 0.6824,  ..., 0.7294, 0.7216, 0.7255],\n",
      "         [0.5137, 0.4784, 0.4980,  ..., 0.7216, 0.7176, 0.7294]],\n",
      "\n",
      "        [[0.3686, 0.3922, 0.4039,  ..., 0.0078, 0.0196, 0.0157],\n",
      "         [0.3451, 0.4392, 0.4157,  ..., 0.0157, 0.0314, 0.0314],\n",
      "         [0.3294, 0.4824, 0.4275,  ..., 0.0353, 0.0510, 0.0549],\n",
      "         ...,\n",
      "         [0.1255, 0.1294, 0.2275,  ..., 0.8039, 0.7804, 0.7725],\n",
      "         [0.1490, 0.3569, 0.4000,  ..., 0.7922, 0.7725, 0.7686],\n",
      "         [0.2118, 0.1804, 0.2118,  ..., 0.7843, 0.7686, 0.7765]]])\n"
     ]
    }
   ],
   "source": [
    "#creatures_data = torchvision.datasets.ImageFolder(creatures_data_path, transform=torchvision.transforms.ToTensor())\n",
    "#items_data = torchvision.datasets.ImageFolder(items_data_path, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "images_data = torchvision.datasets.ImageFolder(images_data_path, transform=torchvision.transforms.ToTensor())\n",
    "len(images_data)\n",
    "for x, y in images_data:\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for x, y in images_data:\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = images_data[0:2]\n",
    "#valid_data = images_loader[0.6*len(images_loader):0.8*len(images_loader)]\n",
    "#test_data = images_loader[0.8*len(images_loader):]\n",
    "\n",
    "sample_num = len(images_data)\n",
    "file_idx = list(range(sample_num))\n",
    "test_percentage = 0.1\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    file_idx, test_size=test_percentage, random_state=42)\n",
    "train_valid_sampler = SubsetRandomSampler(train_val_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "\n",
    "sub_sample_num = len(train_valid_sampler)\n",
    "file_idx = list(range(sub_sample_num))\n",
    "valid_percentage = 0.33\n",
    "train_idx, valid_idx = train_test_split(\n",
    "    file_idx, test_size=valid_percentage, random_state=42)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4952,\n",
       " 12573,\n",
       " 1477,\n",
       " 4252,\n",
       " 180,\n",
       " 6399,\n",
       " 7383,\n",
       " 11149,\n",
       " 12619,\n",
       " 12591,\n",
       " 255,\n",
       " 14698,\n",
       " 14102,\n",
       " 6224,\n",
       " 8618,\n",
       " 11620,\n",
       " 4513,\n",
       " 7886,\n",
       " 2631,\n",
       " 107,\n",
       " 3717,\n",
       " 4463,\n",
       " 13878,\n",
       " 416,\n",
       " 9757,\n",
       " 12439,\n",
       " 8280,\n",
       " 6375,\n",
       " 14361,\n",
       " 2265,\n",
       " 17226,\n",
       " 5079,\n",
       " 13199,\n",
       " 7346,\n",
       " 4606,\n",
       " 13325,\n",
       " 12589,\n",
       " 16781,\n",
       " 12402,\n",
       " 16906,\n",
       " 2621,\n",
       " 2030,\n",
       " 15544,\n",
       " 39,\n",
       " 5903,\n",
       " 17110,\n",
       " 6243,\n",
       " 3508,\n",
       " 13867,\n",
       " 5981,\n",
       " 12273,\n",
       " 2980,\n",
       " 3437,\n",
       " 8262,\n",
       " 7441,\n",
       " 14027,\n",
       " 17059,\n",
       " 3544,\n",
       " 8144,\n",
       " 2867,\n",
       " 4442,\n",
       " 9975,\n",
       " 10944,\n",
       " 12480,\n",
       " 13488,\n",
       " 9749,\n",
       " 6392,\n",
       " 1878,\n",
       " 5371,\n",
       " 7800,\n",
       " 8917,\n",
       " 9173,\n",
       " 15445,\n",
       " 9354,\n",
       " 16315,\n",
       " 15246,\n",
       " 2729,\n",
       " 14573,\n",
       " 13198,\n",
       " 11750,\n",
       " 8180,\n",
       " 17587,\n",
       " 15356,\n",
       " 3076,\n",
       " 5678,\n",
       " 17524,\n",
       " 12974,\n",
       " 14610,\n",
       " 12227,\n",
       " 5297,\n",
       " 2727,\n",
       " 14694,\n",
       " 3836,\n",
       " 36,\n",
       " 9451,\n",
       " 4187,\n",
       " 675,\n",
       " 3633,\n",
       " 1555,\n",
       " 3529,\n",
       " 3979,\n",
       " 16456,\n",
       " 15599,\n",
       " 3708,\n",
       " 8629,\n",
       " 17045,\n",
       " 11097,\n",
       " 5775,\n",
       " 15376,\n",
       " 5384,\n",
       " 15171,\n",
       " 3820,\n",
       " 13486,\n",
       " 17422,\n",
       " 15367,\n",
       " 14396,\n",
       " 1746,\n",
       " 6886,\n",
       " 17404,\n",
       " 12381,\n",
       " 8327,\n",
       " 1052,\n",
       " 4038,\n",
       " 5834,\n",
       " 3405,\n",
       " 3705,\n",
       " 17234,\n",
       " 3955,\n",
       " 5352,\n",
       " 3375,\n",
       " 3428,\n",
       " 10325,\n",
       " 2507,\n",
       " 7553,\n",
       " 4681,\n",
       " 15589,\n",
       " 210,\n",
       " 17177,\n",
       " 10199,\n",
       " 4747,\n",
       " 9972,\n",
       " 13845,\n",
       " 9457,\n",
       " 6959,\n",
       " 1670,\n",
       " 2320,\n",
       " 3495,\n",
       " 10610,\n",
       " 16804,\n",
       " 6914,\n",
       " 13951,\n",
       " 13665,\n",
       " 7245,\n",
       " 13866,\n",
       " 14756,\n",
       " 13003,\n",
       " 484,\n",
       " 11917,\n",
       " 10429,\n",
       " 5828,\n",
       " 16950,\n",
       " 5437,\n",
       " 8374,\n",
       " 7878,\n",
       " 11191,\n",
       " 11299,\n",
       " 2640,\n",
       " 1250,\n",
       " 15010,\n",
       " 13270,\n",
       " 5117,\n",
       " 16740,\n",
       " 7816,\n",
       " 8568,\n",
       " 10125,\n",
       " 4861,\n",
       " 2303,\n",
       " 2652,\n",
       " 9304,\n",
       " 12764,\n",
       " 4244,\n",
       " 718,\n",
       " 6177,\n",
       " 13309,\n",
       " 17129,\n",
       " 12988,\n",
       " 10656,\n",
       " 16125,\n",
       " 12239,\n",
       " 11187,\n",
       " 738,\n",
       " 7753,\n",
       " 16933,\n",
       " 15629,\n",
       " 6053,\n",
       " 10255,\n",
       " 1973,\n",
       " 2556,\n",
       " 17435,\n",
       " 5436,\n",
       " 8062,\n",
       " 8585,\n",
       " 2770,\n",
       " 12554,\n",
       " 2329,\n",
       " 14836,\n",
       " 10306,\n",
       " 8581,\n",
       " 17516,\n",
       " 1275,\n",
       " 7444,\n",
       " 9240,\n",
       " 14847,\n",
       " 4819,\n",
       " 15666,\n",
       " 10901,\n",
       " 6138,\n",
       " 2264,\n",
       " 318,\n",
       " 1536,\n",
       " 14587,\n",
       " 9574,\n",
       " 6134,\n",
       " 1369,\n",
       " 740,\n",
       " 6595,\n",
       " 7138,\n",
       " 1188,\n",
       " 2065,\n",
       " 9720,\n",
       " 9920,\n",
       " 12058,\n",
       " 7582,\n",
       " 2527,\n",
       " 15571,\n",
       " 3487,\n",
       " 6044,\n",
       " 15944,\n",
       " 8270,\n",
       " 7685,\n",
       " 9044,\n",
       " 14982,\n",
       " 6077,\n",
       " 1233,\n",
       " 6834,\n",
       " 10214,\n",
       " 13385,\n",
       " 6113,\n",
       " 16443,\n",
       " 11762,\n",
       " 6757,\n",
       " 12529,\n",
       " 9477,\n",
       " 3488,\n",
       " 1291,\n",
       " 4919,\n",
       " 6408,\n",
       " 16949,\n",
       " 6260,\n",
       " 8607,\n",
       " 12937,\n",
       " 8193,\n",
       " 15091,\n",
       " 15651,\n",
       " 10108,\n",
       " 9226,\n",
       " 16213,\n",
       " 562,\n",
       " 13857,\n",
       " 8827,\n",
       " 15141,\n",
       " 5765,\n",
       " 9926,\n",
       " 3702,\n",
       " 16765,\n",
       " 2407,\n",
       " 4377,\n",
       " 3611,\n",
       " 9441,\n",
       " 7051,\n",
       " 6518,\n",
       " 15448,\n",
       " 2910,\n",
       " 17352,\n",
       " 5734,\n",
       " 15024,\n",
       " 5712,\n",
       " 3166,\n",
       " 7172,\n",
       " 6202,\n",
       " 13463,\n",
       " 7077,\n",
       " 9195,\n",
       " 7606,\n",
       " 2218,\n",
       " 13633,\n",
       " 1955,\n",
       " 8730,\n",
       " 15208,\n",
       " 13352,\n",
       " 15287,\n",
       " 4786,\n",
       " 1399,\n",
       " 6818,\n",
       " 12809,\n",
       " 13587,\n",
       " 13505,\n",
       " 3068,\n",
       " 9780,\n",
       " 12066,\n",
       " 2234,\n",
       " 14181,\n",
       " 10952,\n",
       " 15996,\n",
       " 5306,\n",
       " 6994,\n",
       " 11773,\n",
       " 4549,\n",
       " 10979,\n",
       " 15123,\n",
       " 16525,\n",
       " 6927,\n",
       " 10961,\n",
       " 7562,\n",
       " 3817,\n",
       " 10438,\n",
       " 13624,\n",
       " 5327,\n",
       " 1434,\n",
       " 17687,\n",
       " 12897,\n",
       " 14631,\n",
       " 12953,\n",
       " 11647,\n",
       " 341,\n",
       " 17443,\n",
       " 15678,\n",
       " 8394,\n",
       " 12022,\n",
       " 14581,\n",
       " 5473,\n",
       " 3378,\n",
       " 9386,\n",
       " 5180,\n",
       " 2777,\n",
       " 10200,\n",
       " 428,\n",
       " 1437,\n",
       " 10054,\n",
       " 12450,\n",
       " 8951,\n",
       " 7205,\n",
       " 4692,\n",
       " 17450,\n",
       " 3478,\n",
       " 4504,\n",
       " 7014,\n",
       " 10069,\n",
       " 16046,\n",
       " 7153,\n",
       " 65,\n",
       " 6832,\n",
       " 10782,\n",
       " 16783,\n",
       " 15802,\n",
       " 9066,\n",
       " 14511,\n",
       " 5045,\n",
       " 12478,\n",
       " 7702,\n",
       " 2865,\n",
       " 5364,\n",
       " 9857,\n",
       " 7344,\n",
       " 6730,\n",
       " 5113,\n",
       " 13290,\n",
       " 15093,\n",
       " 6025,\n",
       " 9331,\n",
       " 1691,\n",
       " 7359,\n",
       " 10931,\n",
       " 1421,\n",
       " 9194,\n",
       " 14507,\n",
       " 17144,\n",
       " 819,\n",
       " 15584,\n",
       " 1660,\n",
       " 6562,\n",
       " 1843,\n",
       " 14399,\n",
       " 10124,\n",
       " 7600,\n",
       " 1285,\n",
       " 13815,\n",
       " 4303,\n",
       " 15551,\n",
       " 1941,\n",
       " 16703,\n",
       " 1091,\n",
       " 7475,\n",
       " 73,\n",
       " 3397,\n",
       " 11000,\n",
       " 14133,\n",
       " 4192,\n",
       " 11461,\n",
       " 15450,\n",
       " 10024,\n",
       " 9530,\n",
       " 5064,\n",
       " 4387,\n",
       " 17304,\n",
       " 7070,\n",
       " 13055,\n",
       " 16809,\n",
       " 14965,\n",
       " 13662,\n",
       " 6185,\n",
       " 782,\n",
       " 16188,\n",
       " 16708,\n",
       " 3963,\n",
       " 3937,\n",
       " 7517,\n",
       " 14086,\n",
       " 7254,\n",
       " 1110,\n",
       " 4414,\n",
       " 16486,\n",
       " 10440,\n",
       " 16888,\n",
       " 12514,\n",
       " 1825,\n",
       " 3146,\n",
       " 14682,\n",
       " 15428,\n",
       " 14230,\n",
       " 144,\n",
       " 15791,\n",
       " 6192,\n",
       " 10221,\n",
       " 3514,\n",
       " 13552,\n",
       " 6071,\n",
       " 11575,\n",
       " 6387,\n",
       " 12314,\n",
       " 11178,\n",
       " 10596,\n",
       " 9831,\n",
       " 1830,\n",
       " 11495,\n",
       " 4181,\n",
       " 17104,\n",
       " 1219,\n",
       " 15864,\n",
       " 5957,\n",
       " 6390,\n",
       " 11061,\n",
       " 9300,\n",
       " 6836,\n",
       " 10250,\n",
       " 7019,\n",
       " 1055,\n",
       " 6759,\n",
       " 594,\n",
       " 8665,\n",
       " 16227,\n",
       " 11729,\n",
       " 13252,\n",
       " 5824,\n",
       " 14600,\n",
       " 4702,\n",
       " 13444,\n",
       " 8066,\n",
       " 9469,\n",
       " 7319,\n",
       " 12250,\n",
       " 13715,\n",
       " 17529,\n",
       " 14583,\n",
       " 17143,\n",
       " 5382,\n",
       " 11665,\n",
       " 10802,\n",
       " 10182,\n",
       " 4974,\n",
       " 81,\n",
       " 14622,\n",
       " 532,\n",
       " 958,\n",
       " 472,\n",
       " 8978,\n",
       " 2290,\n",
       " 13576,\n",
       " 12465,\n",
       " 13984,\n",
       " 14274,\n",
       " 12696,\n",
       " 2899,\n",
       " 12138,\n",
       " 3954,\n",
       " 4403,\n",
       " 17560,\n",
       " 15449,\n",
       " 13844,\n",
       " 16745,\n",
       " 6512,\n",
       " 8121,\n",
       " 8744,\n",
       " 5353,\n",
       " 17618,\n",
       " 7791,\n",
       " 17628,\n",
       " 13887,\n",
       " 9452,\n",
       " 8696,\n",
       " 6535,\n",
       " 12593,\n",
       " 10934,\n",
       " 15474,\n",
       " 8899,\n",
       " 6954,\n",
       " 3724,\n",
       " 1264,\n",
       " 4867,\n",
       " 4600,\n",
       " 15573,\n",
       " 9736,\n",
       " 5100,\n",
       " 4371,\n",
       " 16166,\n",
       " 10491,\n",
       " 4374,\n",
       " 1272,\n",
       " 4087,\n",
       " 7125,\n",
       " 13294,\n",
       " 10027,\n",
       " 5770,\n",
       " 1897,\n",
       " 15278,\n",
       " 13227,\n",
       " 6355,\n",
       " 17554,\n",
       " 14327,\n",
       " 1458,\n",
       " 12530,\n",
       " 12125,\n",
       " 15610,\n",
       " 11702,\n",
       " 3761,\n",
       " 3585,\n",
       " 715,\n",
       " 13520,\n",
       " 16368,\n",
       " 1118,\n",
       " 7267,\n",
       " 1287,\n",
       " 5557,\n",
       " 12705,\n",
       " 6740,\n",
       " 6896,\n",
       " 2316,\n",
       " 1417,\n",
       " 11193,\n",
       " 7676,\n",
       " 15804,\n",
       " 14868,\n",
       " 6234,\n",
       " 994,\n",
       " 8741,\n",
       " 4292,\n",
       " 16465,\n",
       " 7046,\n",
       " 2636,\n",
       " 16381,\n",
       " 296,\n",
       " 1864,\n",
       " 15378,\n",
       " 8209,\n",
       " 7920,\n",
       " 1145,\n",
       " 15724,\n",
       " 14000,\n",
       " 16249,\n",
       " 16737,\n",
       " 4829,\n",
       " 9293,\n",
       " 12804,\n",
       " 12766,\n",
       " 12126,\n",
       " 7852,\n",
       " 10873,\n",
       " 7273,\n",
       " 8136,\n",
       " 1835,\n",
       " 10009,\n",
       " 14911,\n",
       " 14221,\n",
       " 14560,\n",
       " 12104,\n",
       " 16427,\n",
       " 5852,\n",
       " 10871,\n",
       " 17677,\n",
       " 13736,\n",
       " 477,\n",
       " 14427,\n",
       " 14031,\n",
       " 4190,\n",
       " 6826,\n",
       " 7061,\n",
       " 9395,\n",
       " 13765,\n",
       " 642,\n",
       " 14395,\n",
       " 4462,\n",
       " 14165,\n",
       " 13180,\n",
       " 14634,\n",
       " 1793,\n",
       " 1692,\n",
       " 7976,\n",
       " 5430,\n",
       " 9806,\n",
       " 493,\n",
       " 10879,\n",
       " 4253,\n",
       " 14059,\n",
       " 1463,\n",
       " 14799,\n",
       " 10529,\n",
       " 13497,\n",
       " 6879,\n",
       " 12189,\n",
       " 13244,\n",
       " 13169,\n",
       " 2287,\n",
       " 16760,\n",
       " 16922,\n",
       " 11804,\n",
       " 965,\n",
       " 444,\n",
       " 13926,\n",
       " 6348,\n",
       " 10017,\n",
       " 438,\n",
       " 11724,\n",
       " 12773,\n",
       " 16606,\n",
       " 10830,\n",
       " 10174,\n",
       " 7309,\n",
       " 5261,\n",
       " 467,\n",
       " 17021,\n",
       " 3751,\n",
       " 7646,\n",
       " 756,\n",
       " 7699,\n",
       " 2688,\n",
       " 5186,\n",
       " 9544,\n",
       " 13713,\n",
       " 3958,\n",
       " 915,\n",
       " 8732,\n",
       " 4511,\n",
       " 1374,\n",
       " 15532,\n",
       " 12235,\n",
       " 17154,\n",
       " 9509,\n",
       " 8131,\n",
       " 13834,\n",
       " 11113,\n",
       " 537,\n",
       " 9143,\n",
       " 7530,\n",
       " 5215,\n",
       " 8898,\n",
       " 17341,\n",
       " 15291,\n",
       " 16855,\n",
       " 9730,\n",
       " 11772,\n",
       " 2186,\n",
       " 12386,\n",
       " 15283,\n",
       " 17457,\n",
       " 12893,\n",
       " 1432,\n",
       " 13952,\n",
       " 11505,\n",
       " 8158,\n",
       " 6270,\n",
       " 332,\n",
       " 3030,\n",
       " 2102,\n",
       " 12799,\n",
       " 16317,\n",
       " 3891,\n",
       " 10567,\n",
       " 10427,\n",
       " 1995,\n",
       " 2952,\n",
       " 8922,\n",
       " 7908,\n",
       " 16607,\n",
       " 5403,\n",
       " 13052,\n",
       " 5292,\n",
       " 14129,\n",
       " 17111,\n",
       " 13018,\n",
       " 4925,\n",
       " 9186,\n",
       " 6244,\n",
       " 17272,\n",
       " 14644,\n",
       " 2447,\n",
       " 15060,\n",
       " 16560,\n",
       " 17584,\n",
       " 14731,\n",
       " 1115,\n",
       " 16177,\n",
       " 8500,\n",
       " 11636,\n",
       " 16196,\n",
       " 5145,\n",
       " 14956,\n",
       " 11465,\n",
       " 4339,\n",
       " 9543,\n",
       " 9995,\n",
       " 15917,\n",
       " 11549,\n",
       " 5070,\n",
       " 5825,\n",
       " 14547,\n",
       " 15950,\n",
       " 14619,\n",
       " 13304,\n",
       " 8583,\n",
       " 15697,\n",
       " 16365,\n",
       " 134,\n",
       " 10111,\n",
       " 11988,\n",
       " 17642,\n",
       " 8709,\n",
       " 799,\n",
       " 14773,\n",
       " 427,\n",
       " 12049,\n",
       " 8196,\n",
       " 4687,\n",
       " 8354,\n",
       " 11272,\n",
       " 2538,\n",
       " 5992,\n",
       " 1255,\n",
       " 9320,\n",
       " 10855,\n",
       " 8689,\n",
       " 4801,\n",
       " 2724,\n",
       " 8852,\n",
       " 12469,\n",
       " 3131,\n",
       " 10845,\n",
       " 1926,\n",
       " 13723,\n",
       " 15895,\n",
       " 3037,\n",
       " 8612,\n",
       " 7063,\n",
       " 12362,\n",
       " 9357,\n",
       " 17240,\n",
       " 12309,\n",
       " 15226,\n",
       " 8610,\n",
       " 8194,\n",
       " 6672,\n",
       " 7554,\n",
       " 3746,\n",
       " 17287,\n",
       " 5369,\n",
       " 5922,\n",
       " 12405,\n",
       " 17202,\n",
       " 7834,\n",
       " 12947,\n",
       " 15019,\n",
       " 3157,\n",
       " 6018,\n",
       " 4096,\n",
       " 13734,\n",
       " 7453,\n",
       " 16799,\n",
       " 6857,\n",
       " 14107,\n",
       " 15771,\n",
       " 3503,\n",
       " 14071,\n",
       " 11725,\n",
       " 14620,\n",
       " 8774,\n",
       " 10194,\n",
       " 2314,\n",
       " 17455,\n",
       " 11866,\n",
       " 8261,\n",
       " 2018,\n",
       " 7387,\n",
       " 8457,\n",
       " 17696,\n",
       " 7534,\n",
       " 15293,\n",
       " 7110,\n",
       " 8097,\n",
       " 5226,\n",
       " 12622,\n",
       " 4540,\n",
       " 15354,\n",
       " 14159,\n",
       " 6011,\n",
       " 10894,\n",
       " 6693,\n",
       " 4126,\n",
       " 3639,\n",
       " 10945,\n",
       " 8408,\n",
       " 11958,\n",
       " 11432,\n",
       " 4080,\n",
       " 7729,\n",
       " 3448,\n",
       " 4962,\n",
       " 10510,\n",
       " 43,\n",
       " 17,\n",
       " 797,\n",
       " 5467,\n",
       " 1883,\n",
       " 13073,\n",
       " 12778,\n",
       " 2372,\n",
       " 16087,\n",
       " 8854,\n",
       " 10127,\n",
       " 16931,\n",
       " 6167,\n",
       " 4094,\n",
       " 17330,\n",
       " 480,\n",
       " 10390,\n",
       " 14891,\n",
       " 13806,\n",
       " 12272,\n",
       " 10609,\n",
       " 1331,\n",
       " 3930,\n",
       " 13356,\n",
       " 3305,\n",
       " 4355,\n",
       " 15118,\n",
       " 443,\n",
       " 11775,\n",
       " 6931,\n",
       " 8937,\n",
       " 14707,\n",
       " 1420,\n",
       " 5205,\n",
       " 15294,\n",
       " 12385,\n",
       " 16981,\n",
       " 1709,\n",
       " 1063,\n",
       " 430,\n",
       " 16870,\n",
       " 13094,\n",
       " 2328,\n",
       " 227,\n",
       " 7770,\n",
       " 577,\n",
       " 4233,\n",
       " 16397,\n",
       " 684,\n",
       " 5220,\n",
       " 8515,\n",
       " 3785,\n",
       " 9276,\n",
       " 5794,\n",
       " 761,\n",
       " 3588,\n",
       " 7613,\n",
       " 796,\n",
       " 7048,\n",
       " 6829,\n",
       " 14061,\n",
       " 14684,\n",
       " 12134,\n",
       " 16073,\n",
       " 15779,\n",
       " 11496,\n",
       " 8700,\n",
       " 6037,\n",
       " 6431,\n",
       " 433,\n",
       " 528,\n",
       " 2758,\n",
       " 5688,\n",
       " 14040,\n",
       " 11887,\n",
       " 1654,\n",
       " 1292,\n",
       " 16775,\n",
       " 14567,\n",
       " 591,\n",
       " 7364,\n",
       " 6840,\n",
       " 5137,\n",
       " 14455,\n",
       " 2392,\n",
       " 6613,\n",
       " 2221,\n",
       " 12238,\n",
       " 14265,\n",
       " 12957,\n",
       " 15441,\n",
       " 1278,\n",
       " 7139,\n",
       " 13077,\n",
       " 11688,\n",
       " 15539,\n",
       " 17606,\n",
       " 7149,\n",
       " 5697,\n",
       " 12107,\n",
       " 6635,\n",
       " 13804,\n",
       " 13897,\n",
       " 16473,\n",
       " 16362,\n",
       " 9505,\n",
       " 2847,\n",
       " 13473,\n",
       " 9432,\n",
       " 5081,\n",
       " 8528,\n",
       " 17672,\n",
       " 15199,\n",
       " 15473,\n",
       " 23,\n",
       " 7315,\n",
       " 7150,\n",
       " 4635,\n",
       " 14745,\n",
       " 11959,\n",
       " 14496,\n",
       " 3540,\n",
       " 12747,\n",
       " 17339,\n",
       " 2383,\n",
       " 5977,\n",
       " 1160,\n",
       " 15680,\n",
       " 5955,\n",
       " 15436,\n",
       " 9688,\n",
       " 15170,\n",
       " 7899,\n",
       " 8013,\n",
       " 2768,\n",
       " 17387,\n",
       " 7591,\n",
       " 1573,\n",
       " 6695,\n",
       " 14903,\n",
       " 16488,\n",
       " 13256,\n",
       " 13840,\n",
       " 17010,\n",
       " 5376,\n",
       " 16027,\n",
       " 13739,\n",
       " 1329,\n",
       " 7820,\n",
       " 3916,\n",
       " 3772,\n",
       " 5484,\n",
       " 6081,\n",
       " 15197,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1967"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images_loader = torch.utils.data.DataLoader(images_data, batch_size=10, shuffle=True)\n",
    "# train_loader = torch.utils.data.DataLoader(images_data, sampler=train_sampler, batch_size=10)\n",
    "train_loader = torch.utils.data.DataLoader(images_data)\n",
    "# train_loader = torch.utils.data.DataLoader(train_idx, sampler=train_sampler, batch_size=10, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(images_data, sampler=valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(images_data, sampler=test_sampler)\n",
    "\n",
    "print(len(train_loader))\n",
    "len(valid_loader)\n",
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11859\n",
      "5841\n",
      "1967\n",
      "19667\n"
     ]
    }
   ],
   "source": [
    "# for x, y in images_loader:\n",
    "#     print(x.shape)\n",
    "#     print(y)\n",
    "#     break\n",
    "print(len(train_sampler))\n",
    "print(len(valid_sampler))\n",
    "print(len(test_sampler))\n",
    "print(len(train_sampler)+len(valid_sampler)+len(test_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "alexnet = torchvision.models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_feature = []\n",
    "for img, y in train_loader:\n",
    "    # print(img.shape, end=\" \")\n",
    "    features = alexnet.features(img).detach()\n",
    "    # print(features.shape)\n",
    "    train_data_feature.append((features, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_feature = []\n",
    "for img, y in valid_loader:\n",
    "    features = alexnet.features(img).detach()\n",
    "    valid_data_feature.append((features, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_feature = []\n",
    "for img, y in test_loader:\n",
    "    features = alexnet.features(img).detach()\n",
    "    test_data_feature.append((features, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(1 * 256 * 6 * 6, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 256 * 6 * 6)\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, batch_size=64, weight_decay=0.0,\n",
    "          optimizer=\"sgd\", learning_rate=0.1, momentum=0.3,\n",
    "          data_shuffle=True, num_epochs=3, checkpoint_path=None):\n",
    "    # loss function\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer\n",
    "    assert optimizer in (\"sgd\", \"adam\")\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                              lr=learning_rate,\n",
    "                              momentum=momentum,\n",
    "                              weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(),\n",
    "                               lr=learning_rate,\n",
    "                               weight_decay=weight_decay)\n",
    "    # track learning curve\n",
    "    iters, losses, train_accs, val_accs = [], [], [], []\n",
    "    # training\n",
    "    n = 0  # the number of iterations (for plotting)\n",
    "    for epoch in range(num_epochs):\n",
    "        # training data\n",
    "        train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=data_shuffle)\n",
    "        for imgs, labels in iter(train_loader):\n",
    "            if imgs.size()[0] < batch_size:\n",
    "                continue\n",
    "\n",
    "            model.train()  # annotate model for training\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, labels.float())\n",
    "            print(out)\n",
    "            print(labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save the current training information\n",
    "            iters.append(n)\n",
    "            # compute *average* loss\n",
    "            single_loss = float(loss)/batch_size\n",
    "            losses.append(single_loss)\n",
    "            train_acc = get_accuracy(model, train_data)\n",
    "            val_acc = get_accuracy(model, val_data)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            n += 1\n",
    "\n",
    "            if checkpoint_path is not None:\n",
    "                torch.save(model.state_dict(), checkpoint_path.format(n))\n",
    "\n",
    "            print(\"Iter %d. [Loss %.9f] [Val Acc %.0f%%] [Train Acc %.0f%%]\" %\n",
    "                  (n, single_loss, val_acc * 100, train_acc * 100))\n",
    "    return iters, losses, train_accs, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data):\n",
    "    # print(data)\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=256)\n",
    "\n",
    "    model.eval()\n",
    "    # print(len(loader))\n",
    "    correct, total = 0, 0 \n",
    "    for imgs, labels in loader:\n",
    "        output = model(imgs)\n",
    "        # print(f\"{imgs.shape=} {labels.shape=}\")\n",
    "        \n",
    "        \n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here.\n",
    "# ploting curve\n",
    "def plot_learning_curve(iters, losses, train_accs, val_accs):\n",
    "    \"\"\"\n",
    "    Plot the learning curve.\n",
    "    \"\"\"\n",
    "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
    "    plt.plot(iters, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
    "    plt.plot(iters, train_accs, label=\"Train\")\n",
    "    plt.plot(iters, val_accs, label=\"Validation\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "print(input.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/bing/Desktop/413_a3/413_final_project.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=2'>3</a>\u001b[0m test_data_new \u001b[39m=\u001b[39m test_data_feature\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=4'>5</a>\u001b[0m mlp \u001b[39m=\u001b[39m MLP()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=5'>6</a>\u001b[0m learning_curve_info \u001b[39m=\u001b[39m train(mlp,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=6'>7</a>\u001b[0m                             train_data_new,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=7'>8</a>\u001b[0m                             valid_data_new,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=8'>9</a>\u001b[0m                             weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=9'>10</a>\u001b[0m                             optimizer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39madam\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=10'>11</a>\u001b[0m                             learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=11'>12</a>\u001b[0m                             num_epochs\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=12'>13</a>\u001b[0m                             checkpoint_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./mlp/ckpt-\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.pk\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000021?line=13'>14</a>\u001b[0m plot_learning_curve(\u001b[39m*\u001b[39mlearning_curve_info)\n",
      "\u001b[1;32m/Users/bing/Desktop/413_a3/413_final_project.ipynb Cell 17'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, batch_size, weight_decay, optimizer, learning_rate, momentum, data_shuffle, num_epochs, checkpoint_path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000017?line=30'>31</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()  \u001b[39m# annotate model for training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000017?line=31'>32</a>\u001b[0m out \u001b[39m=\u001b[39m model(imgs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000017?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, labels\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000017?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bing/Desktop/413_a3/413_final_project.ipynb#ch0000017?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(labels\u001b[39m.\u001b[39mfloat())\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1161'>1162</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1162'>1163</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1163'>1164</a>\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/loss.py?line=1164'>1165</a>\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/functional.py?line=2993'>2994</a>\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/functional.py?line=2994'>2995</a>\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/functional.py?line=2995'>2996</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "train_data_new = train_data_feature\n",
    "valid_data_new = valid_data_feature\n",
    "test_data_new = test_data_feature\n",
    "\n",
    "mlp = MLP()\n",
    "learning_curve_info = train(mlp,\n",
    "                            train_data_new,\n",
    "                            valid_data_new,\n",
    "                            weight_decay=0.1,\n",
    "                            optimizer=\"adam\",\n",
    "                            learning_rate=0.001,\n",
    "                            num_epochs=6,\n",
    "                            checkpoint_path='./mlp/ckpt-{}.pk')\n",
    "plot_learning_curve(*learning_curve_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
