{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will_\\miniconda3\\envs\\csc413\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step1 Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "creature_imgs, creature_captions =dataloader.get_torch_creature_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step2 Process the data**\n",
    "Turn both the image and captions in to embeddings,\\\n",
    "For images we are using **VGG16** to obtain the features\n",
    "For captions we are using **GloVe** embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "creature_tokens = [i.replace(\".\", \" . \").replace(\",\", \" , \").replace(\";\", \" ; \").replace(\"?\", \" ? \").lower().split() for i in creature_captions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Load glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Get the mean of all embedding vectors, we will use this mean as the embedding of all unseen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = glove.vectors.mean(dim=0)\n",
    "glove_vector_with_unk = torch.cat((glove.vectors,mean.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(glove_vector_with_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Get the vector representation of all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## obtain the index of all words in the caption\n",
    "emb_creature_captions =[] \n",
    "for caption in creature_tokens:\n",
    "    emb_creature_captions.append(torch.Tensor([glove.stoi[i] if i in glove.stoi.keys() else 400000 for i in caption]).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pad the captions\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padded_emb_creature_captions = pad_sequence(emb_creature_captions,batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## obtain the embedding\n",
    "glove_emb = nn.Embedding.from_pretrained(glove_vector_with_unk)\n",
    "target = glove_emb(padded_emb_creature_captions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Load VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = torchvision.models.vgg16(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Get the features from VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vgg.features(creature_imgs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512, 7, 7])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_layer = nn.Transformer(128,8,batch_first=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512, 7, 7])\n",
      "torch.Size([5, 34, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the feature number of src and tgt must be equal to d_model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Documents\\SynologyDrive\\School\\Year 3\\CSC413\\Assignments\\project\\CSC413-final-project\\models.ipynb Cell 22'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Documents/SynologyDrive/School/Year%203/CSC413/Assignments/project/CSC413-final-project/models.ipynb#ch0000046?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(inp\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Documents/SynologyDrive/School/Year%203/CSC413/Assignments/project/CSC413-final-project/models.ipynb#ch0000046?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(target\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Documents/SynologyDrive/School/Year%203/CSC413/Assignments/project/CSC413-final-project/models.ipynb#ch0000046?line=3'>4</a>\u001b[0m transformer_layer(features,target)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\csc413\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\csc413\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:139\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/transformer.py?line=135'>136</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe batch number of src and tgt must be equal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/transformer.py?line=137'>138</a>\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/transformer.py?line=138'>139</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/transformer.py?line=140'>141</a>\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(src, mask\u001b[39m=\u001b[39msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/transformer.py?line=141'>142</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/transformer.py?line=142'>143</a>\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/Will_/miniconda3/envs/csc413/lib/site-packages/torch/nn/modules/transformer.py?line=143'>144</a>\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: the feature number of src and tgt must be equal to d_model"
     ]
    }
   ],
   "source": [
    "inp = features\n",
    "print(inp.shape)\n",
    "print(target.shape)\n",
    "transformer_layer(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,dropout_rate, input_dimensions):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        positional_embedding = torch.zeros(input_dimensions,7)\n",
    "        even = torch.arange(0,input_dimensions,2)\n",
    "        odd = torch.arange(1,input_dimensions,2)\n",
    "        position = torch.arange(7)\n",
    "        denominator = torch.float_power(10000,even/input_dimensions)\n",
    "        positional_embedding[0::2] = torch.sin(position.unsqueeze(0)/denominator.unsqueeze(1))\n",
    "        positional_embedding[1::2] = torch.cos(position.unsqueeze(0)/denominator.unsqueeze(1))\n",
    "        horizontal_positional_embedding = positional_embedding\n",
    "        vertical_positional_embedding = positional_embedding\n",
    "        self.positional_embedding = horizontal_positional_embedding.unsqueeze(1) + vertical_positional_embedding.unsqueeze(2)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.positional_embedding.unsqueeze(0)\n",
    "        return self.dropout(x)       \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512, 7, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class caption_transformer(nn.Module):\n",
    "    def __init__(self,num_heads):\n",
    "        super(caption_transformer,self).__init__()\n",
    "        self.cnn_emb = vgg.features\n",
    "        self.cnn_layer = nn.Conv2d(512,128,1)\n",
    "        self.word_emb = nn.Embedding.from_pretrained(glove_vector_with_unk)\n",
    "        self.fc_layer = nn.Linear(50,128)      \n",
    "        self.transformer_layer = nn.Transformer(128,num_heads,batch_first=True)  \n",
    "        self.fc_layer2 = nn.Linear(128,50)\n",
    "        self.positional_embedding = PositionalEncoding(0.1,128)\n",
    "        \n",
    "    def forward(self, inp, target):\n",
    "        # embed the image\n",
    "        emb_inp = torch.relu(self.cnn_layer(inp))\n",
    "        # positional embedding\n",
    "        emb_inp = torch.relu(self.positional_embedding(emb_inp))\n",
    "        # embed the text\n",
    "        emb_target = torch.relu(self.fc_layer(target))\n",
    "        N = emb_inp.shape[0]\n",
    "        dim = emb_inp.shape[1] \n",
    "        emb_inp = emb_inp.view(N,dim,-1)\n",
    "        emb_inp = torch.transpose(emb_inp, 1,2)\n",
    "        out = self.transformer_layer(emb_inp,emb_target)\n",
    "        out = torch.relu(self.fc_layer2(out))\n",
    "        return out\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_target = target[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transformer =  caption_transformer(8)\n",
    "out = my_transformer(features,small_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2171,  0.4651, -0.4676,  0.1008,  1.0135,  0.7484, -0.5310, -0.2626,\n",
       "         0.1681,  0.1318, -0.2491, -0.4419, -0.2174,  0.5100,  0.1345, -0.4314,\n",
       "        -0.0312,  0.2067, -0.7814, -0.2015, -0.0974,  0.1609, -0.6184, -0.1850,\n",
       "        -0.1246, -2.2526, -0.2232,  0.5043,  0.3226,  0.1531,  3.9636, -0.7136,\n",
       "        -0.6701,  0.2839,  0.2174,  0.1443,  0.2593,  0.2343,  0.4274, -0.4445,\n",
       "         0.1381,  0.3697, -0.6429,  0.0241, -0.0393, -0.2604,  0.1202, -0.0438,\n",
       "         0.4101,  0.1796])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.mediabynumbers.com'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_closest_words(vec, n=5):\n",
    "    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n",
    "    for idx, difference in lst[1:n+1]: \t\t\t\t\t       # take the top n\n",
    "        return glove.itos[idx]\n",
    "\n",
    "print_closest_words(out[0][0].detach().numpy(), n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_from_vector(emb_word):\n",
    "    dists = torch.norm(glove.vectors -emb_word, dim=1)\n",
    "    return glove.itos[torch.argmax(dists)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_vector(emb_vectors):\n",
    "    sentences = []\n",
    "    for sentence_vector in emb_vectors:\n",
    "        sentences.append([get_word_from_vector(word) for word in sentence_vector])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0][0] in glove.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['non-families',\n",
       "  'non-families',\n",
       "  '202-383-7824',\n",
       "  'non-families',\n",
       "  '202-383-7824',\n",
       "  '202-383-7824',\n",
       "  '202-383-7824',\n",
       "  'non-families',\n",
       "  '202-383-7824',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  '202-383-7824',\n",
       "  '202-383-7824',\n",
       "  '202-383-7824',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families',\n",
       "  'non-families']]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentences_from_vector(target[0].unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1a9e723f9c0de23dd099ee83cbbfd5eb4ef0e34f00ddb9199c5e9d5c945f09f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csc413')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
